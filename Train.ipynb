{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20c07aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e62f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login into huggingface\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb3dd117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['lyrics', 'title'],\n",
       "        num_rows: 30681\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "songs = load_dataset('csv', data_files='./data/genius-expertiste_clean/songs.csv')\n",
    "songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42eec5cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['lyrics', 'title'],\n",
       "        num_rows: 24544\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['lyrics', 'title'],\n",
       "        num_rows: 3068\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['lyrics', 'title'],\n",
       "        num_rows: 3069\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create train, validation, test splits (80/10/10)\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "songs = songs['train'].train_test_split(test_size=0.2)\n",
    "songs_validation_test = songs['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "songs = DatasetDict({\n",
    "    'train': songs['train'],\n",
    "    'test': songs_validation_test['train'],\n",
    "    'validation': songs_validation_test['test'],\n",
    "    \n",
    "})\n",
    "\n",
    "songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe332584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics:   [Intro] You know, sometimes I think God is playing a little game with me Looking down from heaven, laughing, and trying to see how much I can take Because the way things go, it's like a joke Nobody's had more shots at the moon, and missed, than me  [Verse 1] It's like I got my life stuck, stuck on rewind Trying to make something of myself, life's got something else in mind I'm fighting a losing game and I'm biding my time You won't be my man, do I understand? No, stop I don't want to hear another word about your 'Why not's' I bought your bullshit all before, now you trying to sell me more, babe?  [Chorus] Man who makes a beast out of himself got nothing to lose Sold my soul long ago, nothing left to choose I'm tired, tired of singing the blues I'm tired  [Verse 2] I'm walking down the streets, you wouldn't know what I was thinking It's just another white girl day, hey ribbons in my hair, and I am sinking down A double life, a sordid past, and I am drinking now I want to be bad, you say you were glad to see me, shut up I don't want to know another thing you think you know about me I'm not who you think I am, smiling, but I ain't happy  [Chorus] Man who makes a beast out of himself got nothing to lose Sold my soul long ago, nothing left to choose I'm tired, tired of singing the blues I'm tired  [Bridge] Oh, I'm so far gone, I can't turn off my mind It's like a ticking time bomb Trying to kill me from inside Haunting day and night Is there no remedy for memory?  [Verse 3] I've felt this way before I've known the depths of utter desperation My boyfriend says I'm like an old movie star with loneliness as my occupation I know that he's right but I don't care tonight I want to be bad, I want to be bad, I can't stop I keep running around the same town, knocking you down, I'm fucked How can I get out when there's nowhere to go for miles around?  [Chorus] A man who makes a beast out of himself got nothing to lose Sold my soul long ago, nothing left to choose I'm tired, tired of singing the blues I'm tired  \n",
      "\n",
      "\n",
      "Title: tired of singing the blues\n"
     ]
    }
   ],
   "source": [
    "# looking at a training sample\n",
    "\n",
    "print('Lyrics: {}'.format(songs['train'][3]['lyrics']))\n",
    "print('\\n')\n",
    "print('Title: {}'.format(songs['train'][3]['title']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8b533",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a99571d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "C:\\Program Files\\anaconda3\\envs\\huggingface\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# first load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"google/mt5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "211cf492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs and Attention\n",
      "{'input_ids': [65801, 336, 3869, 714, 12554, 309, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "\n",
      "Tokenization Method\n",
      "['▁Wow', '▁I', '▁love', '▁this', '▁Song', '!', '</s>']\n",
      "\n",
      "\n",
      "Max Input Lenght: 1000000000000000019884624838656\n"
     ]
    }
   ],
   "source": [
    "# testing out the tokenizer\n",
    "print('IDs and Attention')\n",
    "inputs = tokenizer('Wow I love this Song!')\n",
    "print(inputs)\n",
    "\n",
    "# check tokenizer\n",
    "print('\\n')\n",
    "print('Tokenization Method')\n",
    "print(tokenizer.convert_ids_to_tokens(inputs.input_ids))\n",
    "print('\\n')\n",
    "# check max length of input\n",
    "print('Max Input Lenght: {}'.format(tokenizer.model_max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "358b5b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f31fb263aa48819921cb879499e8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24544 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527819f1e2c74c79a44d158f6a202226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4f9ba40b5f428a9ef0128e314bd2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3069 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# specify limit for input and output\n",
    "\n",
    "max_input_length = 300\n",
    "max_target_length = 10\n",
    "\n",
    "# define function to map to train,validation, and test datasets\n",
    "def preprocess_function(examples):\n",
    "    # feeding into tokenizer produces token ids and attention\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"lyrics\"], max_length=max_input_length, truncation=True,\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        examples[\"title\"], max_length=max_target_length, truncation=True\n",
    "    )\n",
    "    \n",
    "    # added additional label feature\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# map to each dataset\n",
    "tokenized_data = songs.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "539403fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "\tDataset({\n",
      "    features: ['lyrics', 'title'],\n",
      "    num_rows: 24544\n",
      "})\n",
      "\n",
      "\n",
      "After:\n",
      "\tDataset({\n",
      "    features: ['lyrics', 'title', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 24544\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# example of change applied to all datasets\n",
    "\n",
    "print('Before:\\n\\t{}'.format(songs['train']))\n",
    "print('\\n')\n",
    "print('After:\\n\\t{}'.format(tokenized_data['train']))\n",
    "\n",
    "# 3 extra columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0f58b9",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80fb9a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model \n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a14202b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters and other arguments\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "num_train_epochs = 5\n",
    "# show the training loss with every epoch\n",
    "logging_steps = len(tokenized_data[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-genius\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    # number of checkpoints to save\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    # generate summaries during evaluation\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42e59866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define compute metrics function\n",
    "\n",
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "   \n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # extract the median scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 3) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "886fda31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data collator for dynamic padding\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14a0f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove original text columns (we already have them but tokenized)\n",
    "\n",
    "tokenized_data = tokenized_data.remove_columns(\n",
    "    songs[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7071be45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jose\\Desktop\\genius_summarization\\mt5-small-finetuned-genius is already a clone of https://huggingface.co/miscjose/mt5-small-finetuned-genius. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "WARNING:huggingface_hub.repository:C:\\Users\\Jose\\Desktop\\genius_summarization\\mt5-small-finetuned-genius is already a clone of https://huggingface.co/miscjose/mt5-small-finetuned-genius. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "# instantiate the trainer\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9539b728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\anaconda3\\envs\\huggingface\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3835' max='3835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3835/3835 2:35:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.505000</td>\n",
       "      <td>3.590725</td>\n",
       "      <td>13.877800</td>\n",
       "      <td>6.473100</td>\n",
       "      <td>13.816600</td>\n",
       "      <td>13.874800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.912600</td>\n",
       "      <td>3.195448</td>\n",
       "      <td>22.406400</td>\n",
       "      <td>12.403000</td>\n",
       "      <td>22.331500</td>\n",
       "      <td>22.401200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.527500</td>\n",
       "      <td>3.077275</td>\n",
       "      <td>23.450400</td>\n",
       "      <td>13.318400</td>\n",
       "      <td>23.388900</td>\n",
       "      <td>23.443200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.386800</td>\n",
       "      <td>3.058105</td>\n",
       "      <td>24.836700</td>\n",
       "      <td>14.395300</td>\n",
       "      <td>24.717200</td>\n",
       "      <td>24.791200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.326000</td>\n",
       "      <td>3.041686</td>\n",
       "      <td>24.785300</td>\n",
       "      <td>14.520300</td>\n",
       "      <td>24.696000</td>\n",
       "      <td>24.731200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3835, training_loss=4.331565504807692, metrics={'train_runtime': 9309.4485, 'train_samples_per_second': 13.182, 'train_steps_per_second': 0.412, 'total_flos': 3.802040745984e+16, 'train_loss': 4.331565504807692, 'epoch': 5.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12979bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.0416862964630127,\n",
       " 'eval_rouge1': 24.7853,\n",
       " 'eval_rouge2': 14.5203,\n",
       " 'eval_rougeL': 24.696,\n",
       " 'eval_rougeLsum': 24.7312,\n",
       " 'eval_runtime': 46.9993,\n",
       " 'eval_samples_per_second': 65.299,\n",
       " 'eval_steps_per_second': 2.043,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check metrics\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1173ddc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ad65ab35b14e7c8ca1f87f6cec27f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/miscjose/mt5-small-finetuned-genius\n",
      "   26a7cec..b9acf17  main -> main\n",
      "\n",
      "WARNING:huggingface_hub.repository:To https://huggingface.co/miscjose/mt5-small-finetuned-genius\n",
      "   26a7cec..b9acf17  main -> main\n",
      "\n",
      "To https://huggingface.co/miscjose/mt5-small-finetuned-genius\n",
      "   b9acf17..fa4b49c  main -> main\n",
      "\n",
      "WARNING:huggingface_hub.repository:To https://huggingface.co/miscjose/mt5-small-finetuned-genius\n",
      "   b9acf17..fa4b49c  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/miscjose/mt5-small-finetuned-genius/commit/b9acf172c0a2940bac201ac03c0c03c2a2d7cadb'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push model to hub\n",
    "\n",
    "trainer.push_to_hub(commit_message=\"Training complete\", tags=\"summarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04be779",
   "metadata": {},
   "source": [
    "# Using the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94709231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c83e162c014498d897261bcd50febc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/773 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\anaconda3\\envs\\huggingface\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jose\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd1c0d844594db981329e238248f47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef81db81f0694e8a9b3078a88747e41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a010a8567744c8b9614dc356778b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/303 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ae8d96d5334a7db197a3513115aff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af138d27e490419ea2fbc8438aa42460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faad829eb64c46cca7bd583cac11a5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "hub_model_id = \"miscjose/mt5-small-finetuned-genius\"\n",
    "summarizer = pipeline(\"summarization\", model=hub_model_id)\n",
    "# or\n",
    "# summarizer = pipeline(\"summarization\", tokenizer=tokenizer, config=args, model=model, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d2e2150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run some examples with the test set \n",
    "def print_summary(idx):\n",
    "    lyrics = songs[\"test\"][idx][\"lyrics\"]\n",
    "    title = songs[\"test\"][idx][\"title\"]\n",
    "    summary = summarizer(songs[\"test\"][idx][\"lyrics\"])[0][\"summary_text\"]\n",
    "    print('Lyrics: {}\\n'.format(lyrics))\n",
    "    print('Title: {}\\n'.format(title))\n",
    "    print('Summary: {}\\n'.format(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efb88fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics:   [Verse 1: Teyana Taylor] Shawty got potential, but he don't need a sponsor You should see his goons, more niggas than a concert Body like Teyana, stomach lookin' proper Eyes half closed cause he smokin' on that ganja Hol' up, hol' up Hard denims and cardigans, they all rugby He my lil bad boy, Sean Puffy Givin' me stacks, a rack T take that Smokin on that James Brown, this the payback I be his hood girl, I put that grind in him So inked up I can write my rhymes with 'em He give me all of his, but let me roll with mine And the shoes spiked up like a porcupine He love my Harlem ass, the way my swag pop A real bad bitch, never needed ass shots Two door coupe, all-white, whole thang And when I see him I'm like honey in that cocaine  [Chorus] Bad boy, real when I need a wrap And his only competition is the IRS Bad boy, a real one, I need that And his only competition is the IRS Make money money, make money money money (Make money money, make money money money) Now everybody say Take money money, take money money money (Take money money, take money money money) IRS  [Verse 2: Honey Cocaine] Yo, well he's a bad boy, but it feels good though I'm out rappin', while he chillin' in the hood, yo All the girls want the money, I don't need shit 'Cause I can do my own work on some queen shit He know he hot shit, money in his pockets Swag out the world, they see him and he's a topic He beat the kitty up, a dog like YG Them Jordan's on fire, jewels icy (ugh) Him stupid is not likely 'Cause all my guys hood smart, I like 'em just like me Now we down at the club with some weed smoke (weed smoke) And a G knows a G 'cause a G knows Hear them others say somethin', you just awesome Bonnie and Clyde through the gang cause we bossin' His motto 'Thug Life,' he run with them bad toys But it's all good, I told you he's a he's a, he's a  [Chorus] Bad boy, real when I need a wrap And his only competition is the IRS Bad boy, a real one, I need that And his only competition is the IRS Make money money, make money money money (Make money money, make money money money) Now everybody say Take money money, take money money money (Take money money, take money money money) IRS  [Outro: Teyana & Honey Cocaine] He my lil bad boy, bad boy Bad boy, bad boy Bad boy, bad boy He my lil bad boy, bad boy Bad boy, bad boy Bad boy, bad boy Hold up Well he's a bad boy, but it feels good though Good though, good though, good though, good though Well he's a bad boy, but it feels good though Good though, good though, good though, good though  \n",
      "\n",
      "Title: bad boy\n",
      "\n",
      "Summary: bad boy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check one example\n",
    "print_summary(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09db04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:huggingface]",
   "language": "python",
   "name": "conda-env-huggingface-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
